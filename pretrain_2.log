======================================================================
PYR PRETRAINING Phase 2 - Narrative Fluency
======================================================================
Loading tokenizer from: ./pyr-135m-base-1\checkpoint-154000
Loading model from: ./pyr-135m-base-1\checkpoint-154000
Model parameters: 135.3M

Loading filtered RoyalRoad dataset from disk: ./clean_royalroad_chapters
   Train: 158,173 samples
   Eval:  1,598 samples

======================================================================
STARTING PYR PRETRAINING PHASE 2
======================================================================
  0%|                                                                                                                                                                                                                                                                                  | 0/3708 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 4.5937, 'grad_norm': 1.125, 'learning_rate': 2e-05, 'epoch': 0.08}
{'loss': 3.4444, 'grad_norm': 0.58984375, 'learning_rate': 2e-05, 'epoch': 0.16}
{'loss': 3.1215, 'grad_norm': 0.4296875, 'learning_rate': 2e-05, 'epoch': 0.24}
{'loss': 3.0437, 'grad_norm': 0.56640625, 'learning_rate': 2e-05, 'epoch': 0.32}
{'loss': 3.0164, 'grad_norm': 0.32421875, 'learning_rate': 2e-05, 'epoch': 0.4}
{'loss': 2.9874, 'grad_norm': 0.32421875, 'learning_rate': 2e-05, 'epoch': 0.49}
{'loss': 2.9676, 'grad_norm': 0.341796875, 'learning_rate': 2e-05, 'epoch': 0.57}
{'loss': 2.9547, 'grad_norm': 0.333984375, 'learning_rate': 2e-05, 'epoch': 0.65}
{'loss': 2.9369, 'grad_norm': 0.3046875, 'learning_rate': 2e-05, 'epoch': 0.73}
{'loss': 2.9332, 'grad_norm': 0.287109375, 'learning_rate': 2e-05, 'epoch': 0.81}
{'eval_loss': 2.4565184116363525, 'eval_runtime': 1288.3947, 'eval_samples_per_second': 1.24, 'eval_steps_per_second': 0.155, 'epoch': 0.81}
{'loss': 2.9248, 'grad_norm': 0.375, 'learning_rate': 2e-05, 'epoch': 0.89}
{'loss': 2.9249, 'grad_norm': 0.36328125, 'learning_rate': 2e-05, 'epoch': 0.97}
{'loss': 2.9188, 'grad_norm': 0.33203125, 'learning_rate': 2e-05, 'epoch': 1.05}
{'loss': 2.9159, 'grad_norm': 0.298828125, 'learning_rate': 2e-05, 'epoch': 1.13}
{'loss': 2.9147, 'grad_norm': 0.298828125, 'learning_rate': 2e-05, 'epoch': 1.21}
{'loss': 2.9077, 'grad_norm': 0.28125, 'learning_rate': 2e-05, 'epoch': 1.29}
{'loss': 2.9025, 'grad_norm': 0.302734375, 'learning_rate': 2e-05, 'epoch': 1.38}
{'loss': 2.9034, 'grad_norm': 0.283203125, 'learning_rate': 2e-05, 'epoch': 1.46}
{'loss': 2.9028, 'grad_norm': 0.33984375, 'learning_rate': 2e-05, 'epoch': 1.54}
{'loss': 2.9027, 'grad_norm': 0.349609375, 'learning_rate': 2e-05, 'epoch': 1.62}
{'eval_loss': 2.4283950328826904, 'eval_runtime': 220.3438, 'eval_samples_per_second': 7.252, 'eval_steps_per_second': 0.908, 'epoch': 1.62}
{'loss': 2.892, 'grad_norm': 0.2890625, 'learning_rate': 2e-05, 'epoch': 1.7}
{'loss': 2.8922, 'grad_norm': 0.3203125, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 2.89, 'grad_norm': 0.357421875, 'learning_rate': 2e-05, 'epoch': 1.86}
{'loss': 2.8903, 'grad_norm': 0.287109375, 'learning_rate': 2e-05, 'epoch': 1.94}
{'loss': 2.886, 'grad_norm': 0.279296875, 'learning_rate': 2e-05, 'epoch': 2.02}
{'loss': 2.885, 'grad_norm': 0.361328125, 'learning_rate': 2e-05, 'epoch': 2.1}
{'loss': 2.8853, 'grad_norm': 0.29296875, 'learning_rate': 2e-05, 'epoch': 2.18}
{'loss': 2.8842, 'grad_norm': 0.294921875, 'learning_rate': 2e-05, 'epoch': 2.27}
{'loss': 2.8838, 'grad_norm': 0.337890625, 'learning_rate': 2e-05, 'epoch': 2.35}
{'loss': 2.8809, 'grad_norm': 0.322265625, 'learning_rate': 2e-05, 'epoch': 2.43}
{'eval_loss': 2.4178409576416016, 'eval_runtime': 221.3165, 'eval_samples_per_second': 7.22, 'eval_steps_per_second': 0.904, 'epoch': 2.43}
{'loss': 2.8843, 'grad_norm': 0.328125, 'learning_rate': 2e-05, 'epoch': 2.51}
{'loss': 2.8777, 'grad_norm': 0.30859375, 'learning_rate': 2e-05, 'epoch': 2.59}
{'loss': 2.8893, 'grad_norm': 0.3046875, 'learning_rate': 2e-05, 'epoch': 2.67}
{'loss': 2.8769, 'grad_norm': 0.2890625, 'learning_rate': 2e-05, 'epoch': 2.75}
{'loss': 2.8852, 'grad_norm': 0.28515625, 'learning_rate': 2e-05, 'epoch': 2.83}
{'loss': 2.8804, 'grad_norm': 0.341796875, 'learning_rate': 2e-05, 'epoch': 2.91}
{'loss': 2.8768, 'grad_norm': 0.32421875, 'learning_rate': 2e-05, 'epoch': 2.99}
{'train_runtime': 169894.5512, 'train_samples_per_second': 2.793, 'train_steps_per_second': 0.022, 'train_loss': 2.9770052584758213, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3708/3708 [47:11:34<00:00, 45.82s/it]

TRAINING COMPLETE!
Model saved to: ./pyr-135m-base-2

======================================================================
PHASE 2 PRETRAINING COMPLETE
======================================================================
